# MachineLearning-Python

### [02 资料](02-Machine-Learning-Basics)


pattern recognition and ML

Introduction to Machine Learning with Python

Zero-to-AI

### [03 Jupyter Notebook Basics](03-Jupyter-Notebook-Basics)

%run、%time等命令，快捷键的使用

### [04 kNN](04-kNN/)

python实现kNN算法与相关处理

### [05 线性回归模型](05-regression/)
python实现线性回归，MAE RMSE R-Square
线性回归模型的可解释性

### [06 梯度下降法](06-梯度下降法)

基于搜索的最优化方法

**梯度下降法**：对某个cost function，需要找到数值最小的位置，利用求导（微分）获取数值减小的方向并移动，不断重复上述过程，就可获得最优解

​	线性回归问题：正规方程解

​	局部最优解问题：多次计算，设定不同的初始值

​	梯度过大：可以对梯度进行归一化，仅获取方向

​	向量化：减少计算时间

**随机梯度下降**：当m很大时，每次计算所有样本的梯度会耗费大量时间，因此每次只随机选择一个样本，学习率需要先大后小，可以模拟退火思想，学习率= a/(i_iters + b)，引入两个超参数

**小批量随机梯度下降法**可以综合上述两种方法的优点

### [07  PCA与梯度上升法](07-PCA与梯度上升法)

PCA：principal component analysis，求解一个新的维度（主成分）可以尽可能多的表达原数据的信息

PCA的cost function：降维后的数据之间方差最大，使用梯度上升法求解(方向向量w)

PCA降维：在求取了某个维度的基后，将原数据去除在该维度上的分量，继续求剩下数据的主成分，多次重复以上过程，直到多个主成分达到需要降的维度，将原数据投影至多个主成分维度空间

PCA可以实现去噪、加速计算的效果，测试了手写数字，特征脸，人脸识别的demo

### [08 多项式回归与模型泛化](08-多项式回归与模型泛化)

 [01 什么是多项式回归.ipynb](08-多项式回归与模型泛化\01 什么是多项式回归.ipynb) 

将x<sup>2</sup>作为新特征，使用线性回归思路，可以拟合出非线性关系

 [02 scikit-learn中的多项式回归与pipeline.ipynb](08-多项式回归与模型泛化\02 scikit-learn中的多项式回归与pipeline.ipynb) 

​	使用polyfeature构造指数项

​	pipeline可以将多项式构建、归一化、训练多个步骤整合

 [03 过拟合与欠拟合.ipynb](08-多项式回归与模型泛化\03 过拟合与欠拟合.ipynb) 

​	当指数项的指数过大时，会出现过拟合现象（将噪声也作为特征纳入训练中）

 [04 训练数据集与测试数据集的意义.ipynb](08-多项式回归与模型泛化\04 训练数据集与测试数据集的意义.ipynb) 

​	当模型在训练集上过拟合时，在测试集上会出现较差结果，可以帮助确认拟合的情况

 [05 学习曲线.ipynb](08-多项式回归与模型泛化\05 学习曲线.ipynb) 

​	对比不同的数据量（或其他参数）对训练结果的影响

​	对比训练集、测试集的模型准确率

 [06 验证数据集与交叉验证.ipynb](08-多项式回归与模型泛化\06 验证数据集与交叉验证.ipynb) 

​	更严谨的模型评价方法（测试集的选择），数据也是一种超参数

**k_folds cross validation**: 将数据集分为k份，其中k-1份分别单独训练，第k份进行验证，重复k次，最终结果取均值，可以避免只使用一次时验证数据集中出现异常数据导致评价结果偏差太大

**leave-one-out cross validation** 留一法：分成样本数量m的份数，不受随机影响，计算量大

 [07 偏差方差平衡.ipynb](08-多项式回归与模型泛化\07 偏差方差平衡.ipynb) 

模型误差：偏差（Bias）+方差（Variance) + 不可避免的误差（噪声）

**偏差：**对问题本身的假设不正确，如非线性数据使用了线性回归，欠拟合underfitting

**方差**：数据本身的扰动会较大影响模型，通常是模型使用的太复杂，如高阶多项式回归，过拟合

**高方差算法**：kNN对数据本身敏感，其他非参数学习（不对数据进行假设）

**高偏差算法**：线性回归，参数学习算法（对数据有极强的假设）

大多数算法具有相应的参数，可以调整**偏差和方差**，注意：偏差与方差通常是矛盾的

机器学习主要降低方差的手段：

 	1. 降低模型复杂度
 	2. 减少数据维度；降噪
 	3. 增加样本数量
 	4. 使用验证集
 	5. 正则化

 [08 模型正则化与岭回归.ipynb](08-多项式回归与模型泛化\08 模型正则化与岭回归.ipynb) 

 **模型正则化**

岭回归（Ridge Regression): 在损失函数中添加系数项，使得最终模型中系数项较小

  [09 模型正则化LASSO回归.ipynb](08-多项式回归与模型泛化\09 模型正则化LASSO回归.ipynb) 

LASSO (Least Absolute Shrinkage and Selection Operator) Regression：与岭回归相同，差别在使用绝对值代表系数项大小

LASSO可以帮助进行==特征选择==，系数项的差异使得LASSO在迭代时可以更快的达到0的位置

  [10 范数、正则项与弹性网](08-多项式回归与模型泛化\10 范数、正则项与弹性网.ipynb) 

Ridge、MSE、欧拉距离，这些实际上都是一些度量方式

Lp范数与Lp正则项的概念

注：L0正则的优化是NP难问题

**弹性网 Elastic Net**：结合岭回归与LASSO

### [09 逻辑回归](09-逻辑回归)

回归解决分类问题，计算给出样本特征下的概率p，不能直接支持多分类；由于概率p的值域在[0,1]之间，需要使用sigmoid函数处理回归后得到的数值

1. sigmoid：f(t) = 1/ (1+e<sup>-t</sup>)

2. cost = -ylog(p)-(1-y)log(1-p)

[04 实现逻辑回归算法.ipynb](09-逻辑回归\04 实现逻辑回归算法.ipynb) 

没有数学解，凸函数，可使用梯度下降法计算

[05 决策边界.ipynb](09-逻辑回归\05 决策边界.ipynb) 

对于不规则决策边界，可以通过传递均匀分布在坐标系里的数据，使用模型预测分类后进行渲染来绘制

 [07 逻辑回归的正则项-sklearn中的逻辑回归.ipynb](09-逻辑回归\07 逻辑回归的正则项-sklearn中的逻辑回归.ipynb) 

添加正则项

 [08 OvR与OvO.ipynb](09-逻辑回归\08 OvR与OvO.ipynb) 

多分类问题

 1. One vs Rest

    n个类别进行n次分类，每次将类型分为当前类与其他类，然后通过投票选择最终分类结果

 2. One vs One

    每次只对其中两个类别进行二分类，汇总分类结果进行投票

    10个类别需要进行45次分类

### [10 评价分类结果](10-评价分类结果)

 [精准率和召回率](10-评价分类结果\02 精准率和召回率.ipynb) 

 [F1-score](10-评价分类结果\04 F1-score.ipynb) 

 [ROC曲线](10-评价分类结果\07 ROC曲线.ipynb) 

### [11 SVM](11-SVM)

Hard Margin

Soft Margin：相对于Hard Margin增加了容错值

核函数

高斯核函数(RBF)

SVM解决回归问题



### [12 决策树](12-决策树)



决策树：

* 非参数学习
* 天然可以解决多分类问题
* 可用于回归问题
* 可解释性强

信息熵：

* 一组数据的不确定性与信息熵成正比

基尼系数

决策树的局限性

* 高度依赖调参
* 决策边界比较固定

集成学习：随机森林



